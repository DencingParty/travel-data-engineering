FROM bitnami/spark:3.5.3

USER 0

# 필요한 패키지 설치
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3-pip \
    curl \
    && pip3 install pyspark==3.5.3 \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# 하둡-aws 및 aws-sdk 추가 (S3 연동 설정)
RUN mkdir -p /opt/bitnami/spark/jars && \
    curl -o /opt/bitnami/spark/jars/hadoop-aws-3.3.2.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar && \
    curl -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.11.901.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar

# spark-env.sh 생성 및 ClassPath 설정
RUN cp /opt/bitnami/spark/conf/spark-env.sh.template /opt/bitnami/spark/conf/spark-env.sh && \
    echo "export SPARK_DIST_CLASSPATH=/opt/bitnami/spark/jars/*" >> /opt/bitnami/spark/conf/spark-env.sh

# spark-defaults.conf 설정 (S3A 파일시스템 사용)
RUN mkdir -p /opt/bitnami/spark/conf && \
    echo "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" >> /opt/bitnami/spark/conf/spark-defaults.conf && \
    echo "spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider" >> /opt/bitnami/spark/conf/spark-defaults.conf && \
    echo "spark.driver.extraClassPath /opt/bitnami/spark/jars/*" >> /opt/bitnami/spark/conf/spark-defaults.conf && \
    echo "spark.executor.extraClassPath /opt/bitnami/spark/jars/*" >> /opt/bitnami/spark/conf/spark-defaults.conf

# spark-env.sh 복사 및 권한 설정
COPY ./spark/conf/spark-env.sh /opt/bitnami/spark/conf/spark-env.sh
RUN chmod 755 /opt/bitnami/spark/conf/spark-env.sh && \
    chown -R 1001:1001 /opt/bitnami/spark/conf

# 환경 변수 설정
ENV PYSPARK_PYTHON=python3
ENV SPARK_DIST_CLASSPATH=/opt/bitnami/spark/jars/*

USER 1001
